{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Refreshers\n",
    "\n",
    "Supervised learning in simple terms corresponds to family of algorithms used to optimize the minimization function using already labelled data. If the function (called loss function) is suitably minimized, this becomes an excellent means to predict. The onjective is generally to minimse a defined loss function $ g(\\bar{Y},Y_0)$ where $ Y_0 = f(X,K) $ by optimizing $K$ (some parameter which is used to predict value). $K$ gets optimized during training and remains constant during prediction \n",
    "\n",
    "Most algorithms define $f(X,K)$ as a linear function of $K$, $X$ and/or transformed/ normalized $X$. In classiscal statistical modelling, $X$ was **sometimes** transformed to preserve the statistical assumptions regarding the input variables. For example, Box-Cox transformation was applied to input variables to ensure the residuals followed a normal distribution. Sometimes, transforming $X$ creates a more logical variable than using the variable as it is. For example, if pricing of products such as milk differ from store to store, substituting pricing with discount might be a more useful variable, given people buy products based on discounts *(since its difficult to remember the price is MSRP is not provided)*. More about this later (Add link to this section in future)\n",
    "\n",
    "The above explanation is detailed way of saying **Predict $\\bar{Y}$ by changing K in such a way, that $ g(\\bar{Y},f(X,K))$ is as minimal as possible. When this is achieved, $f(X,K)$ can be substituted for $\\bar{Y}$ in the future**  \n",
    "\n",
    "The reason this has be so complicatedly layed out is to appreciate the role of $g(\\bar{Y},Y_0)$, the loss function. $f(X,K)$ decides how the model is set up. $g(\\bar{Y},Y_0)$ will determine what kind of losses are trying to be minimzed. MSE or $(Y-Y_0)^2$ is a commonly used Loss function, since minimizing this is relatively easy. There are other ways of minimizing as well, discussed probably later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
